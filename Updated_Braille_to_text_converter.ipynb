{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Braille to text converter",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [
        {
          "file_id": "19cLVbAvTfuu0pqaadxpgyMsFxHC3nnNF",
          "timestamp": 1527465087778
        }
      ],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "T20hE89hygNL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This CNN model was trained on our old dataset that included only 78 images. NOTE: this code also has a bunch of run-time errors and tensor errors. We are working on our new model that will be trained on a much larger dataset. "
      ]
    },
    {
      "metadata": {
        "id": "H-ra1DTnR0S6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Install pytorch "
      ]
    },
    {
      "metadata": {
        "id": "ikmT9ejsWLrU",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "cellView": "code",
        "outputId": "278a1cbd-0a08-4eea-c5ba-0e0ff7225cbb",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527438683955,
          "user_tz": 240,
          "elapsed": 129591,
          "user": {
            "displayName": "Ting-Yi Su",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118326931190013634369"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# use k fold cross validation\n",
        "# use cross entropy for classification problems\n",
        "\n",
        "import time\n",
        "import platform\n",
        "import io\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "from matplotlib.pyplot import cm \n",
        "\n",
        "# import KFold from scikit-learn\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "def install_pytorch():\n",
        "    os = platform.system()\n",
        "    if os == \"Linux\":\n",
        "        !pip3 install http://download.pytorch.org/whl/cu90/torch-0.4.0-cp36-cp36m-linux_x86_64.whl\n",
        "    elif os == \"Windows\":\n",
        "        !pip3 install http://download.pytorch.org/whl/cu90/torch-0.4.0-cp36-cp36m-win_amd64.whl \n",
        "    !pip3 install torchvision\n",
        "\n",
        "\n",
        "# Install PyTorch.\n",
        "install_pytorch()\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch==0.4.0 from http://download.pytorch.org/whl/cu90/torch-0.4.0-cp36-cp36m-linux_x86_64.whl\n",
            "\u001b[?25l  Downloading http://download.pytorch.org/whl/cu90/torch-0.4.0-cp36-cp36m-linux_x86_64.whl (566.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 566.4MB 78.1MB/s \n",
            "tcmalloc: large alloc 1073750016 bytes == 0x5cf72000 @  0x7fdd1c5a01c4 0x46d6a4 0x5fcbcc 0x4c494d 0x54f3c4 0x553aaf 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54e4c8\n",
            "\u001b[?25hInstalling collected packages: torch\n",
            "Successfully installed torch-0.4.0\n",
            "Collecting torchvision\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/0d/f00b2885711e08bd71242ebe7b96561e6f6d01fdb4b9dcf4d37e2e13c5e1/torchvision-0.2.1-py2.py3-none-any.whl (54kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 3.7MB/s \n",
            "\u001b[?25hCollecting pillow>=4.1.1 (from torchvision)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5f/4b/8b54ab9d37b93998c81b364557dff9f61972c0f650efa0ceaf470b392740/Pillow-5.1.0-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.0MB 10.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision) (0.4.0)\n",
            "Installing collected packages: pillow, torchvision\n",
            "  Found existing installation: Pillow 4.0.0\n",
            "    Uninstalling Pillow-4.0.0:\n",
            "      Successfully uninstalled Pillow-4.0.0\n",
            "Successfully installed pillow-5.1.0 torchvision-0.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BK-KXPMNRbao",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Enable GPU Usage\n",
        "\n",
        "Models trains much more quickly with GPU. "
      ]
    },
    {
      "metadata": {
        "id": "wvckHX5NktK7",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "turn on GPU\n",
        "use_gpu = torch.cuda.is_available()\n",
        "\n",
        "print(\"GPU Available: {}\".format(use_gpu))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TuhFUrX0O2Se",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ]
    },
    {
      "metadata": {
        "id": "Tw-iCQFfRhS7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Import and read the CSV file "
      ]
    },
    {
      "metadata": {
        "id": "_1kJYrn79Cft",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#import csv dataset\n",
        "import torchvision\n",
        "import csv\n",
        "import os\n",
        "import pandas as pd \n",
        "from urllib import request\n",
        "import requests\n",
        "\n",
        "# Upload and read the csv file from the github repo\n",
        "\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/HelenG123/ai-alliance/master/brailleFinalv2.csv\")\n",
        "\n",
        "#https://raw.githubusercontent.com/HelenG123/ai-alliance/master/braille_data.csv\n",
        "# # Read the CSV file from a local directory\n",
        "# dataset_name = list(dataset.keys())[0]\n",
        "# df = pd.read_csv(io.StringIO(dataset[dataset_name].decode('utf-8')))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S6XGJhcJOvCM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Iterate over the CSV files\n",
        "Create a dictionary of the images that contains the image as a Tensor and its target as a one-hot encoded vector."
      ]
    },
    {
      "metadata": {
        "id": "k6W-EK9_Oz_C",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from scipy import misc\n",
        "from io import BytesIO\n",
        "import urllib\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import urllib.request\n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "data=[]\n",
        "\n",
        "for i, row in df.iterrows():\n",
        "  picture = []\n",
        "  url = row['Labeled Data']\n",
        "  label = row['External ID']\n",
        "  curr_target = target[label[0]]\n",
        "\n",
        "  x = urllib.request.urlopen(url)\n",
        "  resp = x.read()\n",
        "  image = np.array(bytearray(resp), dtype=np.uint8)\n",
        "  image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
        "  # resize image\n",
        "  image = cv2.resize(image, (28, 28))\n",
        "#   image = image.astype(np.float32)/255.0\n",
        "#   image = image.flatten().astype(np.float32)/255.0\n",
        "  image = torch.from_numpy(image)\n",
        "  picture.append(image)\n",
        "  curr_target=torch.Tensor(curr_target)\n",
        "  picture.append(curr_target)\n",
        "  data.append(picture)\n",
        "\n",
        "print(image.shape) # these are the dimensions of our image\n",
        "print(data[0][0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l2GVHtlCPEvi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### k-fold Cross Validation\n",
        "\n",
        "Don't have enough data so need to generate more."
      ]
    },
    {
      "metadata": {
        "id": "gAa8Q1M9yJkY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Copying the same dataset over and over again?"
      ]
    },
    {
      "metadata": {
        "id": "I5eKqcyXPFdF",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "training_set = []\n",
        "test_set = []\n",
        "\n",
        "for i in range(100):\n",
        "  training_set.append(data)\n",
        "  \n",
        "for i in range(30):\n",
        "  test_set.append(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B4Qe0zGryX3J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This k-fold code gives 3 sets of training and test sets."
      ]
    },
    {
      "metadata": {
        "id": "RikI11vfPJAf",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Implement k-fold Cross Validation\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "kf = KFold(n_splits = 3)\n",
        "\n",
        "training_set = []\n",
        "test_set = []\n",
        "\n",
        "# for testing: visualization purposes \n",
        "# We splitted the images id's in two: train and test\n",
        "for train_index, test_index in kf.split(data): \n",
        "  print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
        "\n",
        "for i in range (len(train_index)):\n",
        "  training_set.append(data)\n",
        "\n",
        "for i in range (len(test_index)):\n",
        "  test_set.append(data)\n",
        "\n",
        "  training_set.append(train_index)\n",
        "  test_set.append(test_index)\n",
        "\n",
        "for i in range(0:78)\n",
        "  training_set[i] =  images[i][0]\n",
        "  test_set[i]\n",
        "print(training_set)\n",
        "print()\n",
        "print(test_set)\n",
        "\n",
        "# Use this code if are using k-fold\n",
        "new_training_set = []\n",
        "# finds the position of the \n",
        "for array_i in range(len(training_set)):\n",
        "  for curr_pos_i in range(len(training_set[array_i])):\n",
        "    curr_training_index = training_set[array_i][curr_pos_i]\n",
        "    curr_image = [curr_training_index][0]\n",
        "    curr_target = data[curr_training_index][1]\n",
        "#     print(curr_image)\n",
        "    new_training_set.append(curr_image)\n",
        "    new_training_set.append(curr_target)\n",
        "\n",
        "print(new_training_set)\n",
        "print(new_test_set)\n",
        "    \n",
        "# print(new_training_set[0][0])\n",
        "\n",
        "new_test_set = []\n",
        "for array_i in range(len(test_set)):\n",
        "  for curr_pos_i in range(len(test_set[array_i])):\n",
        "    curr_test_index = test_set[array_i][curr_pos_i]\n",
        "    curr_image = data[curr_test_index][0]\n",
        "    new_test_set.append(curr_image)\n",
        "    new_test_set.append(curr_target)\n",
        "#print(new_test_set), print(len(new_test_set))\n",
        "# To test and display the indexes of the train and test data\n",
        "# for train_index, test_index in kf.split(images):\n",
        "#   print(train_index, test_index)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ikPAbyzkzr3a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Create training and test datasets\n"
      ]
    },
    {
      "metadata": {
        "id": "thE4Nl9nMP0d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Load the dataset\n",
        "\n",
        "Dataloader gives a the object that we can iterate over so that we can enumerate and train our data."
      ]
    },
    {
      "metadata": {
        "id": "jAGps6lEMXVC",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "batch_size = 5\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aBXqSDbRRKtk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Generate the Targets\n",
        "Create a dictionary that contains the one-hot encoding vector for each image in the Braille alphabet.\n"
      ]
    },
    {
      "metadata": {
        "id": "ByQJboIJRIER",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import string\n",
        "alphabet = list(string.ascii_lowercase)\n",
        "\n",
        "target = {}\n",
        "\n",
        "# Initalize a target dict that has the letters as its keys and as its value\n",
        "# an empty one-hot encoding of size 26\n",
        "for letter in alphabet: \n",
        "  target[letter] = [0] * 26\n",
        "\n",
        "# Do the one-hot encoding for each letter now \n",
        "curr_pos = 0 \n",
        "for curr_letter in target.keys():\n",
        "  target[curr_letter][curr_pos] = 1\n",
        "  curr_pos += 1  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zUxdGGHGeajB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Visualize the Image\n",
        "Demonstrate that we can access and display an image from the dataset. \n"
      ]
    },
    {
      "metadata": {
        "id": "UkvrVL6EaJ0Z",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "4c5c3b13-91ed-4557-df71-44781d9f3835",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527438718797,
          "user_tz": 240,
          "elapsed": 387,
          "user": {
            "displayName": "Ting-Yi Su",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118326931190013634369"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Display 'y' in Brailles\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "dd = data[24][0].numpy()\n",
        "print('Braille Target: Y/y')\n",
        "plt.imshow(dd)\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Braille Target: Y/y\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFKCAYAAACU6307AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f340a3174e0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "n7kpWYqcx0Ab",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## CNN Model"
      ]
    },
    {
      "metadata": {
        "id": "zjZ3lalDfUrv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define the Model"
      ]
    },
    {
      "metadata": {
        "id": "ruhzBcNumGl0",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "outputId": "0da1b6a3-a2d6-47c6-f1db-ca0eff57d899",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527441479234,
          "user_tz": 240,
          "elapsed": 306,
          "user": {
            "displayName": "Ting-Yi Su",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118326931190013634369"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# import the nn.Module class\n",
        "import torch.nn as nn\n",
        "\n",
        "# defines the convolutional neural network\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.block1 = nn.Sequential(\n",
        "            #3x28x28\n",
        "            nn.Conv2d(in_channels=3, \n",
        "                      out_channels=16, \n",
        "                      kernel_size=5, \n",
        "                      stride=1, \n",
        "                      padding=2)\n",
        "            #16x28x28\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            #16x14x14\n",
        "        )\n",
        "        #16x14x14\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16, \n",
        "                      out_channels=32, \n",
        "                      kernel_size=5, \n",
        "                      stride=1, \n",
        "                      padding=2)\n",
        "            #32x14x14\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "            #32x7x7\n",
        "        ) \n",
        "        # linearly \n",
        "        self.block3 = nn.Sequential(\n",
        "            nn.Linear(32*7*7, 500),\n",
        "            nn.Linear(500, 300),\n",
        "            nn.Linear(300, 100),\n",
        "            nn.Linear(100, 26)\n",
        "        )\n",
        "        \n",
        "        #1x26\n",
        "    \n",
        "    def forward(self, x): \n",
        "        out = self.block1(x)\n",
        "        out = self.block2(out)\n",
        "        # flatten the dataset\n",
        "        out = out.view(-1, 32*7*7)\n",
        "        out = self.block3(out)\n",
        "        \n",
        "        return out\n",
        "\n",
        "# convolutional neural network model\n",
        "model = CNN()\n",
        "\n",
        "# if using GPU\n",
        "if use_gpu:\n",
        "  # switch model to GPU\n",
        "  model.cuda()\n",
        "\n",
        "# print summary of the neural network model to check if everything is fine. \n",
        "print(model)\n",
        "print(\"# parameter: \", sum([param.nelement() for param in model.parameters()]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CNN(\n",
            "  (block1): Sequential(\n",
            "    (0): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (block2): Sequential(\n",
            "    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (block3): Sequential(\n",
            "    (0): Linear(in_features=1568, out_features=500, bias=True)\n",
            "    (1): Linear(in_features=500, out_features=300, bias=True)\n",
            "    (2): Linear(in_features=300, out_features=100, bias=True)\n",
            "    (3): Linear(in_features=100, out_features=26, bias=True)\n",
            "  )\n",
            ")\n",
            "# parameter:  981574\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "broXSAfX9fma",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Set the learning rate, criterion, & optimizer"
      ]
    },
    {
      "metadata": {
        "id": "0qiBVs22kt0S",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#setting the learning rate\n",
        "learning_rate = 1e-3\n",
        "\n",
        "# Using a variable to store the cross entropy method\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Using a variable to store the optimizer \n",
        "optimizer = torch.optim.Adam(model.parameters(),lr = learning_rate)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CLfmDuBL9lqi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train and Evaluate the data"
      ]
    },
    {
      "metadata": {
        "id": "kvfb94yR9XPK",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "dbbb945e-211c-4205-efda-a69a12a0272b",
        "executionInfo": {
          "status": "error",
          "timestamp": 1527441483222,
          "user_tz": 240,
          "elapsed": 300,
          "user": {
            "displayName": "Ting-Yi Su",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118326931190013634369"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "t0 = time.time()\n",
        "\n",
        "# variable to store the total loss\n",
        "total_loss = []\n",
        "\n",
        "# for loop that iterates over all the epochs\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    \n",
        "    # variables to store/keep track of the loss and number of iterations\n",
        "    train_loss = 0\n",
        "    num_iter = 0\n",
        "    \n",
        "    # train the model\n",
        "    model.train()\n",
        "    \n",
        "    # Iterate over data.\n",
        "    for i, (images, labels) in enumerate(training_set[i]):  \n",
        "      \n",
        "       \n",
        "        print(images.shape)\n",
        "        \n",
        "        # need to permute so that the images are of size 3x28x28 \n",
        "        # essential to be able to feed images into the model\n",
        "        images = images.permute(2,0,1)\n",
        "#         images.unsqueeze_(0)\n",
        "\n",
        "        # if GPU is available \n",
        "        if use_gpu:\n",
        "          # switch tensor type to GPU\n",
        "          images = images.cuda()\n",
        "          labels = labels.cuda()\n",
        "\n",
        "        # Zero the gradient buffer\n",
        "        # resets the gradient after each epoch so that the gradients don't add up\n",
        "        optimizer.zero_grad()  \n",
        "        \n",
        "        # Forward\n",
        "        outputs = model(images)\n",
        "        # calculate the loss\n",
        "        loss = criterion(outputs, labels)\n",
        "        total_loss.append(loss)\n",
        "        # Backward\n",
        "        loss.backward()\n",
        "        \n",
        "        # Optimize\n",
        "        # loops through all parameters and updates weights by using the gradients \n",
        "        optimizer.step()\n",
        "        # update the training loss and number of iterations\n",
        "        train_loss += loss.data[0]\n",
        "        num_iter += 1\n",
        "    \n",
        "    print('Epoch: {}, Loss: {:.4f}'.format(\n",
        "          epoch+1, train_loss/num_iter))\n",
        "    \n",
        "    # evaluate the model\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Iterate over data.\n",
        "    for images, labels in test_set:  \n",
        "       \n",
        "       # if GPU is available \n",
        "       if use_gpu:\n",
        "          # switch tensor type to GPU\n",
        "          images = images.cuda()\n",
        "          labels = labels.cuda()\n",
        "          \n",
        "       # Forward\n",
        "       outputs = model(images)\n",
        "       loss = criterion(outputs, labels)  \n",
        "       _, predicted = torch.max(outputs.data, 1)\n",
        "    \n",
        "       # Statistics\n",
        "       total += labels.size(0)\n",
        "       correct += (predicted == labels).sum()\n",
        "       \n",
        "    print('Accuracy on the test set: {}%'.format(100 * correct / total))\n",
        "tf = time.time()\n",
        "print()\n",
        "print(\"time: {} s\" .format(tf-t0))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 1, 1, 1, 28, 28, 3])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-a0e3adde67c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m#         # if GPU is available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: number of dims don't match in permute"
          ]
        }
      ]
    }
  ]
}