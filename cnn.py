# -*- coding: utf-8 -*-
"""MyFirstCNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jX4tkk5SsYcuIE4uwQZkleNSucqImh78

This CNN model was trained on the MNIST dataset. The MNIST dataset is a database of handwritten digits that is commonly used for training varius image processing systems.

## Import statements
"""

# import all necessary items
import time
import platform
import io
from google.colab import files
import matplotlib.pyplot as plt
from matplotlib.pyplot import cm 

# install pytorch and Torchvision
def install_pytorch():
    os = platform.system()
    if os == "Linux":
        !pip3 install http://download.pytorch.org/whl/cu90/torch-0.4.0-cp36-cp36m-linux_x86_64.whl
    elif os == "Windows":
        !pip3 install http://download.pytorch.org/whl/cu90/torch-0.4.0-cp36-cp36m-win_amd64.whl 
    !pip3 install torchvision
    
install_pytorch()

# now that Pytorch and Torchvision are installed, import the relevant libraries
import torch
import torch.nn as nn
import torch.optim as optim

"""## Dataset

### Download the dataset from PyTorch

Import the MNIST dataset using PyTorch tools (PyTorch has an [MNIST Dataset class](https://pytorch.org/docs/stable/torchvision/datasets.html?highlight=mnist#mnist). Download the MNIST dataset into training and test datasets.
"""

# import the necessary libraries 
import torch
from torchvision.datasets import MNIST
import torchvision.transforms as transforms

# create the training and test datasets
train_dataset = MNIST(root='../data', train=True, transform=transforms.ToTensor(), download=True)

test_dataset = MNIST(root='../data', train=False, transform=transforms.ToTensor(), download=True)

"""### Load the dataset

For training and evaluation purposes, the dataset objects are  wrapped in PyTorch [DataLoader objects](https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader).  The function<b><a href="http://pytorch.org/docs/master/data.html"> torch.utils.data.DataLoader </a></b> which divides the dataset automatically in mini-batches.
"""

batch_size = 100

# Dataloader gives object that you can iterate on 
# will need this to enumerate/train data
train_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size = batch_size, shuffle = True)
test_loader = torch.utils.data.DataLoader(dataset = test_dataset, batch_size = batch_size, shuffle = False)

# checking
d = list(train_loader)
len(d)

"""### Take the train_loader set for the following model"""

x = list(train_loader)[0][0]
x.shape

"""### Generate the Targets
Create a dictionary that contains the one-hot encoding vector for each image in the Braille alphabet.
"""

import string
alphabet = list(string.ascii_lowercase)

target = {}

# Initalize a target dict that has the letters as its keys and as its value
# an empty one-hot encoding of size 26
for letter in alphabet: 
  target[letter] = [0] * 26

# Do the one-hot encoding for each letter now 
curr_pos = 0 
for curr_letter in target.keys():
  target[curr_letter][curr_pos] = 1
  curr_pos += 1

"""## Convolutional Neural Network model

Define the neural network model in the CNN class below. The CNN class will inherit from the [`nn.Module` class](https://pytorch.org/docs/stable/nn.html#module) (this class includes some neural net boilerplate code and magic methods).
"""

# import the nn.Module class
import torch.nn as nn

# defines the convolutional neural network
class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        self.block1 = nn.Sequential(
            #1x28x28
            nn.Conv2d(in_channels=1, 
                      out_channels=16, 
                      kernel_size=5, 
                      stride=1, 
                      padding=2),
            #16x28x28
            nn.MaxPool2d(kernel_size=2),
            #16x14x14
        )
        #16x14x14
        self.block2 = nn.Sequential(
            nn.Conv2d(in_channels=16, 
                      out_channels=32, 
                      kernel_size=5, 
                      stride=1, 
                      padding=2),
            #32x14x14
            nn.MaxPool2d(kernel_size=2)
            #32x7x7
        ) 
        # linearly 
        self.block3 = nn.Sequential(
            nn.Linear(32*7*7, 500),
            nn.Linear(500, 300),
            nn.Linear(300, 100),
            nn.Linear(100, 10)
        )
        
    
    def forward(self, x): 
        out = self.block1(x)
        out = self.block2(out)
        # flatten the dataset
        out = out.view(-1, 32*7*7)
        out = self.block3(out)
        
        return out

# convolutional neural network model
model = CNN()

# print summary of the neural network model to check if everything is fine. 
print(model)
print("# parameter: ", sum([param.nelement() for param in model.parameters()]))


"""### Print out the new shape of the dataset"""

out = model(x)

# shape of dataset before model
print("Previous shape:", x.shape)

# new shape of dataset
print("New shape:", out.shape)

"""### Define the loss

Cross-entropy or log loss will be used as the loss function. This loss function is ideal for classification models
"""

criterion = nn.CrossEntropyLoss()

"""### Set the learning rate

Set the learning rate on which to train the model.
"""

learning_rate = 1e-3

"""### Optimizer

For this optimizer, SGD (Stochastic Gradient Descent) will be used, using the learning rate defined above.
"""

optimizer = optim.SGD(model.parameters(), lr=learning_rate)

"""## Train & Evaluate the model

Now that the model has been defined, it's time to train & evaluate the model. The model will be trained on epochs of mini-batches (train_loader).

### Train

The code below is trained on 10 epochs (an epoch is a complete representation of the dataset to be learned).

### Evaluate

This is very similar to training, except that the gradient is not computed and the parameters are not updated.
"""

t0 = time.time()

# variable to store the total loss
total_loss = []

# for loop that iterates over all the epochs
num_epochs = 10
for epoch in range(num_epochs):
    
    # variables to store/keep track of the loss and number of iterations
    train_loss = 0
    num_iter = 0
    
    # train the model
    model.train()
    
    # Iterate over data.
    for i, (images, labels) in enumerate(train_loader):  
      
        # Zero the gradient buffer
        # resets the gradient after each epoch so that the gradients don't add up
        optimizer.zero_grad()  
        
        # Forward
        outputs = model(images)
        # calculate the loss
        loss = criterion(outputs, labels)
        total_loss.append(loss)
        # Backward
        loss.backward()
        
        # Optimize
        # loops through all parameters and updates weights by using the gradients 
        optimizer.step()
        # update the training loss and number of iterations
        train_loss += loss.data[0]
        num_iter += 1
    
    print('Epoch: {}, Loss: {:.4f}'.format(
          epoch+1, train_loss/num_iter))
    
    # evaluate the model
    model.eval()

    correct = 0
    total = 0

    # Iterate over data.
    for images, labels in test_loader:  
          
       # Forward
       outputs = model(images)
       loss = criterion(outputs, labels)  
       _, predicted = torch.max(outputs.data, 1)
    
       # Statistics
       total += labels.size(0)
       correct += (predicted == labels).sum()
       
    print('Accuracy on the test set: {}%'.format(100 * correct / total))
tf = time.time()
print()
print("time: {} s" .format(tf-t0))
